---
layout: page
title: User Classification for Smart Glasses: From Authentication to Identification
description: On-device user presence detection using wearable IMU data
img: assets/img/12.jpg
importance: 1
category: work
related_publications: true
---

### ğŸ‘“ Smart Glass Participant Detection  
**A lightweight, on-device model to detect user presence using smart glassesâ€™ motion sensors.**

---

### ğŸš€ Why It Matters  
Wearables continuously generate sensor data â€” but not all of it is meaningful. This project tackles the practical problem of detecting whether a smart glass device is actively being worn. It ensures that downstream applications like human activity recognition (HAR) or user authentication donâ€™t process irrelevant or noisy data.

---

### âš™ï¸ System Overview  

<div class="row">
  <div class="col-sm-12 mt-3">
    {% include figure.liquid path="assets/img/projects/1_project/architecture.png" title="System Pipeline" class="img-fluid rounded z-depth-1" %}
  </div>
</div>

We collect IMU data from smart glasses, extract low-cost features, and classify each segment as "user" or "no-user" â€” all on-device, with real-time capability.

---

### ğŸ§ª Dataset & Gesture Design  

<div class="row">
  <div class="col-sm-12 mt-3">
    {% include figure.liquid path="assets/img/projects/1_project/head_movements.png" title="Gesture Set" class="img-fluid rounded z-depth-1" %}
  </div>
</div>

- **17 participants**, each performing 6 gestures  
- **Device**: Epson Moverio BT-350  
- **Sensors**: Accelerometer, Gyroscope, Rotation Vector, Geomagnetic  
- **Sampling**: 110â€¯Hz (Acc/Gyr), 55â€¯Hz (GeoMag/RotVec)

---

### ğŸ“‰ Sensor Signals: User Variation  

<div class="row">
  <div class="col-sm-12 mt-3">
    {% include figure.liquid path="assets/img/projects/1_project/3dplots.png" title="3D IMU Traces by User" class="img-fluid rounded z-depth-1" %}
  </div>
</div>

Gestures produce distinct patterns across users and sensors â€” a foundation for both authentication and filtering.

---

### ğŸ“Š Results Summary  

<div class="row">
  <div class="col-sm-6 mt-3">
    {% include figure.liquid path="assets/img/projects/1_project/classifiers.png" title="Classifier Comparison (EER %)" class="img-fluid rounded z-depth-1" %}
  </div>
  <div class="col-sm-6 mt-3">
    {% include figure.liquid path="assets/img/projects/1_project/far_frr_eer.png" title="EER/FAR/FRR by Gesture" class="img-fluid rounded z-depth-1" %}
  </div>
</div>

- **Adaboost outperforms** RF, SVM, and MLP across all gestures  
- Triangle gesture with RotVec+GeoMag achieves **1.3% EER**

<div class="row">
  <div class="col-sm-6 mt-3">
    {% include figure.liquid path="assets/img/projects/1_project/participant_based2.png" title="Optimization Impact by User" class="img-fluid rounded z-depth-1" %}
  </div>
  <div class="col-sm-6 mt-3">
    {% include figure.liquid path="assets/img/projects/1_project/eer_results.png" title="Sensor Fusion Results Table" class="img-fluid rounded z-depth-1" %}
  </div>
</div>

- Oversampling + sliding window reduced EER across users  
- Using **only 2 sensors** yields performance comparable to 3â€“4 sensor setups

---

### ğŸ”‘ Key Takeaways

- âœ… **1.3% EER** with triangle gesture  
- âœ… **Only 3 features per signal**: mean, min, max  
- âœ… Runs efficiently on wearable hardware (no deep learning required)  
- âœ… Dataset + logger developed in-house  
- âœ… Supports further steps like authentication & identification  

---

### ğŸ”— Resources  
- ğŸ“ [GitHub Repository](https://github.com/sumeyye-agac/glass-data-participant-detection)  
- ğŸ“„ [Published Paper (SN Computer Science)](https://doi.org/10.1007/s42979-023-02202-4)

**Tags:** `#EdgeAI` `#Wearables` `#SensorFusion` `#IMU` `#BehavioralBiometrics` `#MachineLearning`
