<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>Smart glasses continuously stream motion sensor data, but traditional systems treat all of it the same ‚Äî regardless of whether the device is actually being worn. This can lead to irrelevant or misleading signals being used for downstream tasks such as activity recognition or access control.</p> <p>This project introduces a real-time, on-device system that filters out such noise by determining whether the glasses are actively worn. When worn, it authenticates the rightful user and can even identify them ‚Äî using only IMU data and lightweight machine learning models optimized for embedded hardware.</p> <hr> <h3 id="system-overview">System Overview</h3> <div class="row"> <div class="col-sm-12 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/architecture-480.webp 480w,/assets/img/projects/1_project/architecture-800.webp 800w,/assets/img/projects/1_project/architecture-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/architecture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="System Pipeline" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The system collects motion data from smart glasses, segments it into time windows, and extracts simple time-domain features. Classification is performed at three levels:</p> <ul> <li> <strong>Presence Detection</strong>: Is the device being worn?</li> <li> <strong>Authentication</strong>: Is the current user the authorized one?</li> <li> <strong>Identification</strong>: Which user is wearing the device?</li> </ul> <p>All predictions are made on-device in real time.</p> <hr> <h3 id="dataset--gesture-design">Dataset &amp; Gesture Design</h3> <div class="row"> <div class="col-sm-12 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/head_movements-480.webp 480w,/assets/img/projects/1_project/head_movements-800.webp 800w,/assets/img/projects/1_project/head_movements-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/head_movements.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Head Gesture Set" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>A custom dataset was collected from 17 participants wearing Epson Moverio BT-350 smart glasses. Each participant performed six predefined head gestures (triangle, square, up-down, etc.). Data was gathered from:</p> <ul> <li>4 motion sensors: Accelerometer, Gyroscope, Rotation Vector, Geomagnetic</li> <li>Sampling rates: 110‚ÄØHz (Acc/Gyro), 55‚ÄØHz (GeoMag/RotVec)</li> <li>Signals segmented into 1-second windows with 50% overlap</li> <li>Each window encoded with 108 features (mean, min, max across axes)</li> </ul> <hr> <h3 id="signal-variation-across-users">Signal Variation Across Users</h3> <div class="row"> <div class="col-sm-12 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/3dplots-480.webp 480w,/assets/img/projects/1_project/3dplots-800.webp 800w,/assets/img/projects/1_project/3dplots-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/3dplots.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="3D IMU Patterns" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Users executed the same gestures in distinct ways. This variation became a reliable behavioral signal ‚Äî enabling identity recognition from motion alone.</p> <hr> <h3 id="methods">Methods</h3> <p>A range of models were implemented and compared:</p> <ul> <li> <strong>Preprocessing</strong>: normalization, statistical feature extraction, overlapping windowing</li> <li> <strong>Classifiers</strong>: <ul> <li>Adaboost (most effective overall)</li> <li>Random Forest</li> <li>SVM (RBF and polynomial kernels)</li> <li>MLP</li> <li>Model ensembles (SVM + MLP, etc.)</li> </ul> </li> <li> <strong>Class imbalance</strong> was addressed using SMOTE</li> <li>Evaluations were performed using 10-fold cross-validation on gesture-specific splits</li> </ul> <hr> <h3 id="results">Results</h3> <p>Without relying on deep learning or cloud computation, the system achieved:</p> <ul> <li> <strong>1.3% Equal Error Rate</strong> for user authentication, using only the triangle gesture and two sensors (Rotation Vector + Geomagnetic)</li> <li> <strong>99.3% weighted F1-score</strong> for identifying users across 17 participants, with Random Forest and sensor fusion</li> <li>Sliding window and SMOTE significantly improved generalization</li> <li>High performance was maintained even with just two sensors ‚Äî making it suitable for energy-efficient, on-device deployment</li> </ul> <hr> <h3 id="why-it-matters">Why It Matters</h3> <p>This system proves that reliable user detection and identification can be done entirely on-device, with minimal compute, using handcrafted features. It supports smarter, more secure wearable experiences without the need for continuous cloud connection or power-hungry models.</p> <p>The approach is generalizable to other wearable contexts and was validated through peer-reviewed publication.</p> <hr> <h3 id="-resources">üîó Resources</h3> <ul> <li>üìÅ <a href="https://github.com/sumeyye-agac/glass-data-participant-detection" rel="external nofollow noopener" target="_blank">GitHub Repository</a> </li> <li>üìÑ <a href="https://doi.org/10.1007/s42979-023-02202-4" rel="external nofollow noopener" target="_blank">Published Paper</a> </li> </ul> <hr> <p><strong>Tags:</strong><br> <code class="language-plaintext highlighter-rouge">#UserAuthentication</code> <code class="language-plaintext highlighter-rouge">#EdgeAI</code> <code class="language-plaintext highlighter-rouge">#BehavioralBiometrics</code> <code class="language-plaintext highlighter-rouge">#Wearables</code><br> <code class="language-plaintext highlighter-rouge">#SensorFusion</code> <code class="language-plaintext highlighter-rouge">#IMU</code> <code class="language-plaintext highlighter-rouge">#TimeSeriesClassification</code> <code class="language-plaintext highlighter-rouge">#Human-CenteredAI</code></p> </body></html>