<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h3 id="-smart-glass-participant-detection--">👓 Smart Glass Participant Detection —</h3> <p>layout: page title: User Classification for Smart Glasses: From Authentication to Identification description: IMU-based, on-device user detection system for wearable smart glasses img: assets/img/12.jpg importance: 1 category: work related_publications: true —</p> <h3 id="-user-classification-for-smart-glasses-from-authentication-to-identification">👓 User Classification for Smart Glasses: From Authentication to Identification</h3> <p><strong>A real-time system for detecting user presence, authenticating, and identifying individuals via motion sensor data from smart glasses — optimized for wearable deployment.</strong></p> <hr> <h3 id="-project-overview">🎯 Project Overview</h3> <p>This project addresses a practical challenge in wearable computing: determining whether a smart glass device is actively being worn. The system classifies time-series IMU data to enable presence-aware filtering and supports secure, personalized use through user authentication and identification.</p> <hr> <h3 id="-motivation">💡 Motivation</h3> <p>Smart glasses and similar wearables collect continuous motion data, but not all of it is valid or meaningful. Detecting user presence before running downstream models like HAR or authentication improves data quality, conserves power, and enhances security.</p> <hr> <h3 id="️-system-design">⚙️ System Design</h3> <div class="row"> <div class="col-sm-12 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/architecture-480.webp 480w,/assets/img/projects/1_project/architecture-800.webp 800w,/assets/img/projects/1_project/architecture-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/architecture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="System Pipeline" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The system processes raw IMU signals, extracts time-domain features, and classifies windows as either user or no-user. When a user is present, additional authentication and identification stages can be triggered — all designed for real-time execution on wearable hardware.</p> <hr> <h3 id="-data-collection--gesture-design">🧪 Data Collection &amp; Gesture Design</h3> <div class="row"> <div class="col-sm-12 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/head_movements-480.webp 480w,/assets/img/projects/1_project/head_movements-800.webp 800w,/assets/img/projects/1_project/head_movements-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/head_movements.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Gesture Set" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>17 participants</li> <li>6 head gestures: circle, up-down, tilt, triangle, turn, square</li> <li>Device: Epson Moverio BT-350 smart glasses</li> <li>Sensors: Accelerometer, Gyroscope, Rotation Vector, Geomagnetic</li> <li>Sampling: 110 Hz / 55 Hz</li> </ul> <p>Data was segmented into 1-second windows, with simple features (mean, min, max) computed per axis and sensor — enabling efficient modeling with limited compute.</p> <hr> <h3 id="-signal-patterns-across-users">📉 Signal Patterns Across Users</h3> <div class="row"> <div class="col-sm-12 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/3dplots-480.webp 480w,/assets/img/projects/1_project/3dplots-800.webp 800w,/assets/img/projects/1_project/3dplots-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/3dplots.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="3D IMU Traces by User" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Variability in gesture patterns across users formed the basis for behavioral biometric classification — essential for reliable authentication and identification.</p> <hr> <h3 id="-results">📊 Results</h3> <div class="row"> <div class="col-sm-6 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/classifiers-480.webp 480w,/assets/img/projects/1_project/classifiers-800.webp 800w,/assets/img/projects/1_project/classifiers-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/classifiers.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Classifier Comparison" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/far_frr_eer-480.webp 480w,/assets/img/projects/1_project/far_frr_eer-800.webp 800w,/assets/img/projects/1_project/far_frr_eer-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/far_frr_eer.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="EER / FAR / FRR per Gesture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>Authentication</strong>: Achieved 1.3% EER using triangle gesture and RotVec + GeoMag sensors</li> <li> <strong>Identification</strong>: 99.3% F1-score with Random Forest using 3-sensor fusion</li> <li>Adaboost outperformed other models in most gesture-sensor settings</li> <li>Applied sliding window overlap and SMOTE to handle sequential structure and class imbalance</li> </ul> <div class="row"> <div class="col-sm-6 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/participant_based2-480.webp 480w,/assets/img/projects/1_project/participant_based2-800.webp 800w,/assets/img/projects/1_project/participant_based2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/participant_based2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Error Reduction with Optimizations" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/eer_results-480.webp 480w,/assets/img/projects/1_project/eer_results-800.webp 800w,/assets/img/projects/1_project/eer_results-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/eer_results.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Sensor Fusion Results" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <h3 id="-highlights">✅ Highlights</h3> <ul> <li>End-to-end classification system for <strong>user detection, authentication, and identification</strong> </li> <li>Works in real time using <strong>only time-domain features</strong> (mean, min, max)</li> <li>Demonstrated state-of-the-art performance with <strong>minimal sensor combinations</strong> </li> <li>System optimized for <strong>resource-limited wearable deployment</strong> </li> <li>Research published in <em>SN Computer Science</em> (Springer, 2023)</li> </ul> <hr> <h3 id="-resources">🔗 Resources</h3> <ul> <li>📁 <a href="https://github.com/sumeyye-agac/glass-data-participant-detection" rel="external nofollow noopener" target="_blank">GitHub Repository</a> </li> <li>📄 <a href="https://doi.org/10.1007/s42979-023-02202-4" rel="external nofollow noopener" target="_blank">Published Paper (SN Computer Science)</a> </li> </ul> <p><strong>Tags:</strong> <code class="language-plaintext highlighter-rouge">#MachineLearning</code> <code class="language-plaintext highlighter-rouge">#Wearables</code> <code class="language-plaintext highlighter-rouge">#EdgeAI</code> <code class="language-plaintext highlighter-rouge">#SensorFusion</code> <code class="language-plaintext highlighter-rouge">#IMU</code> <code class="language-plaintext highlighter-rouge">#BehavioralBiometrics</code> <code class="language-plaintext highlighter-rouge">#TimeSeries</code></p> <p><strong>A lightweight, on-device model to detect user presence using smart glasses’ motion sensors.</strong></p> <hr> <h3 id="-why-it-matters">🚀 Why It Matters</h3> <p>Wearables continuously generate sensor data — but not all of it is meaningful. This project tackles the practical problem of detecting whether a smart glass device is actively being worn. It ensures that downstream applications like human activity recognition (HAR) or user authentication don’t process irrelevant or noisy data.</p> <hr> <h3 id="️-system-overview">⚙️ System Overview</h3> <div class="row"> <div class="col-sm-12 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/architecture-480.webp 480w,/assets/img/projects/1_project/architecture-800.webp 800w,/assets/img/projects/1_project/architecture-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/architecture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="System Pipeline" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We collect IMU data from smart glasses, extract low-cost features, and classify each segment as “user” or “no-user” — all on-device, with real-time capability.</p> <hr> <h3 id="-dataset--gesture-design">🧪 Dataset &amp; Gesture Design</h3> <div class="row"> <div class="col-sm-12 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/head_movements-480.webp 480w,/assets/img/projects/1_project/head_movements-800.webp 800w,/assets/img/projects/1_project/head_movements-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/head_movements.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Gesture Set" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>17 participants</strong>, each performing 6 gestures</li> <li> <strong>Device</strong>: Epson Moverio BT-350</li> <li> <strong>Sensors</strong>: Accelerometer, Gyroscope, Rotation Vector, Geomagnetic</li> <li> <strong>Sampling</strong>: 110 Hz (Acc/Gyr), 55 Hz (GeoMag/RotVec)</li> </ul> <hr> <h3 id="-sensor-signals-user-variation">📉 Sensor Signals: User Variation</h3> <div class="row"> <div class="col-sm-12 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/3dplots-480.webp 480w,/assets/img/projects/1_project/3dplots-800.webp 800w,/assets/img/projects/1_project/3dplots-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/3dplots.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="3D IMU Traces by User" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Gestures produce distinct patterns across users and sensors — a foundation for both authentication and filtering.</p> <hr> <h3 id="-results-summary">📊 Results Summary</h3> <div class="row"> <div class="col-sm-6 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/classifiers-480.webp 480w,/assets/img/projects/1_project/classifiers-800.webp 800w,/assets/img/projects/1_project/classifiers-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/classifiers.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Classifier Comparison (EER %)" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/far_frr_eer-480.webp 480w,/assets/img/projects/1_project/far_frr_eer-800.webp 800w,/assets/img/projects/1_project/far_frr_eer-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/far_frr_eer.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="EER/FAR/FRR by Gesture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>Adaboost outperforms</strong> RF, SVM, and MLP across all gestures</li> <li>Triangle gesture with RotVec+GeoMag achieves <strong>1.3% EER</strong> </li> </ul> <div class="row"> <div class="col-sm-6 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/participant_based2-480.webp 480w,/assets/img/projects/1_project/participant_based2-800.webp 800w,/assets/img/projects/1_project/participant_based2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/participant_based2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Optimization Impact by User" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/eer_results-480.webp 480w,/assets/img/projects/1_project/eer_results-800.webp 800w,/assets/img/projects/1_project/eer_results-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/eer_results.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Sensor Fusion Results Table" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Oversampling + sliding window reduced EER across users</li> <li>Using <strong>only 2 sensors</strong> yields performance comparable to 3–4 sensor setups</li> </ul> <hr> <h3 id="-key-takeaways">🔑 Key Takeaways</h3> <ul> <li>✅ <strong>1.3% EER</strong> with triangle gesture</li> <li>✅ <strong>Only 3 features per signal</strong>: mean, min, max</li> <li>✅ Runs efficiently on wearable hardware (no deep learning required)</li> <li>✅ Dataset + logger developed in-house</li> <li>✅ Supports further steps like authentication &amp; identification</li> </ul> <hr> <h3 id="-resources-1">🔗 Resources</h3> <ul> <li>📁 <a href="https://github.com/sumeyye-agac/glass-data-participant-detection" rel="external nofollow noopener" target="_blank">GitHub Repository</a> </li> <li>📄 <a href="https://doi.org/10.1007/s42979-023-02202-4" rel="external nofollow noopener" target="_blank">Published Paper (SN Computer Science)</a> </li> </ul> <p><strong>Tags:</strong> <code class="language-plaintext highlighter-rouge">#EdgeAI</code> <code class="language-plaintext highlighter-rouge">#Wearables</code> <code class="language-plaintext highlighter-rouge">#SensorFusion</code> <code class="language-plaintext highlighter-rouge">#IMU</code> <code class="language-plaintext highlighter-rouge">#BehavioralBiometrics</code> <code class="language-plaintext highlighter-rouge">#MachineLearning</code></p> </body></html>