<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h3 id="-smart-glass-participant-detection">ğŸ‘“ Smart Glass Participant Detection</h3> <p><strong>A lightweight, on-device model to detect user presence using smart glassesâ€™ motion sensors.</strong></p> <hr> <h3 id="-why-it-matters">ğŸš€ Why It Matters</h3> <p>Wearables continuously generate sensor data â€” but not all of it is meaningful. This project tackles the practical problem of detecting whether a smart glass device is actively being worn. It ensures that downstream applications like human activity recognition (HAR) or user authentication donâ€™t process irrelevant or noisy data.</p> <hr> <h3 id="ï¸-system-overview">âš™ï¸ System Overview</h3> <div class="row"> <div class="col-sm-12 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/architecture-480.webp 480w,/assets/img/projects/1_project/architecture-800.webp 800w,/assets/img/projects/1_project/architecture-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/architecture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="System Pipeline" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We collect IMU data from smart glasses, extract low-cost features, and classify each segment as â€œuserâ€ or â€œno-userâ€ â€” all on-device, with real-time capability.</p> <hr> <h3 id="-dataset--gesture-design">ğŸ§ª Dataset &amp; Gesture Design</h3> <div class="row"> <div class="col-sm-12 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/head_movements-480.webp 480w,/assets/img/projects/1_project/head_movements-800.webp 800w,/assets/img/projects/1_project/head_movements-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/head_movements.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Gesture Set" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>17 participants</strong>, each performing 6 gestures</li> <li> <strong>Device</strong>: Epson Moverio BT-350</li> <li> <strong>Sensors</strong>: Accelerometer, Gyroscope, Rotation Vector, Geomagnetic</li> <li> <strong>Sampling</strong>: 110â€¯Hz (Acc/Gyr), 55â€¯Hz (GeoMag/RotVec)</li> </ul> <hr> <h3 id="-sensor-signals-user-variation">ğŸ“‰ Sensor Signals: User Variation</h3> <div class="row"> <div class="col-sm-12 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/3dplots-480.webp 480w,/assets/img/projects/1_project/3dplots-800.webp 800w,/assets/img/projects/1_project/3dplots-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/3dplots.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="3D IMU Traces by User" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Gestures produce distinct patterns across users and sensors â€” a foundation for both authentication and filtering.</p> <hr> <h3 id="-results-summary">ğŸ“Š Results Summary</h3> <div class="row"> <div class="col-sm-6 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/classifiers-480.webp 480w,/assets/img/projects/1_project/classifiers-800.webp 800w,/assets/img/projects/1_project/classifiers-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/classifiers.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Classifier Comparison (EER %)" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/far_frr_eer-480.webp 480w,/assets/img/projects/1_project/far_frr_eer-800.webp 800w,/assets/img/projects/1_project/far_frr_eer-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/far_frr_eer.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="EER/FAR/FRR by Gesture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>Adaboost outperforms</strong> RF, SVM, and MLP across all gestures</li> <li>Triangle gesture with RotVec+GeoMag achieves <strong>1.3% EER</strong> </li> </ul> <div class="row"> <div class="col-sm-6 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/participant_based2-480.webp 480w,/assets/img/projects/1_project/participant_based2-800.webp 800w,/assets/img/projects/1_project/participant_based2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/participant_based2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Optimization Impact by User" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/eer_results-480.webp 480w,/assets/img/projects/1_project/eer_results-800.webp 800w,/assets/img/projects/1_project/eer_results-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/eer_results.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Sensor Fusion Results Table" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Oversampling + sliding window reduced EER across users</li> <li>Using <strong>only 2 sensors</strong> yields performance comparable to 3â€“4 sensor setups</li> </ul> <hr> <h3 id="-key-takeaways">ğŸ”‘ Key Takeaways</h3> <ul> <li>âœ… <strong>1.3% EER</strong> with triangle gesture</li> <li>âœ… <strong>Only 3 features per signal</strong>: mean, min, max</li> <li>âœ… Runs efficiently on wearable hardware (no deep learning required)</li> <li>âœ… Dataset + logger developed in-house</li> <li>âœ… Supports further steps like authentication &amp; identification</li> </ul> <hr> <h3 id="-resources">ğŸ”— Resources</h3> <ul> <li>ğŸ“ <a href="https://github.com/sumeyye-agac/glass-data-participant-detection" rel="external nofollow noopener" target="_blank">GitHub Repository</a> </li> <li>ğŸ“„ <a href="https://doi.org/10.1007/s42979-023-02202-4" rel="external nofollow noopener" target="_blank">Published Paper (SN Computer Science)</a> </li> </ul> <p><strong>Tags:</strong> <code class="language-plaintext highlighter-rouge">#EdgeAI</code> <code class="language-plaintext highlighter-rouge">#Wearables</code> <code class="language-plaintext highlighter-rouge">#SensorFusion</code> <code class="language-plaintext highlighter-rouge">#IMU</code> <code class="language-plaintext highlighter-rouge">#BehavioralBiometrics</code> <code class="language-plaintext highlighter-rouge">#MachineLearning</code></p> </body></html>