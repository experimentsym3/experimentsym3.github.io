<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h3 id="-user-classification-for-smart-glasses-from-authentication-to-identification">👓 User Classification for Smart Glasses: From Authentication to Identification</h3> <p><strong>A real-time system for detecting user presence, authenticating, and identifying individuals via motion sensor data from smart glasses — optimized for wearable deployment.</strong></p> <hr> <h3 id="-project-overview">🎯 Project Overview</h3> <p>This project addresses a practical challenge in wearable computing: determining whether a smart glass device is actively being worn. The system classifies time-series IMU data to enable presence-aware filtering and supports secure, personalized use through user authentication and identification.</p> <hr> <h3 id="-motivation">💡 Motivation</h3> <p>Smart glasses and similar wearables collect continuous motion data, but not all of it is valid or meaningful. Detecting user presence before running downstream models like HAR or authentication improves data quality, conserves power, and enhances security.</p> <hr> <h3 id="️-system-design">⚙️ System Design</h3> <div class="row"> <div class="col-sm-12 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/architecture-480.webp 480w,/assets/img/projects/1_project/architecture-800.webp 800w,/assets/img/projects/1_project/architecture-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/architecture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="System Pipeline" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The system processes raw IMU signals, extracts time-domain features, and classifies windows as either user or no-user. When a user is present, additional authentication and identification stages can be triggered — all designed for real-time execution on wearable hardware.</p> <hr> <h3 id="-data-collection--gesture-design">🧪 Data Collection &amp; Gesture Design</h3> <div class="row"> <div class="col-sm-12 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/head_movements-480.webp 480w,/assets/img/projects/1_project/head_movements-800.webp 800w,/assets/img/projects/1_project/head_movements-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/head_movements.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Gesture Set" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>17 participants</li> <li>6 head gestures: circle, up-down, tilt, triangle, turn, square</li> <li>Device: Epson Moverio BT-350 smart glasses</li> <li>Sensors: Accelerometer, Gyroscope, Rotation Vector, Geomagnetic</li> <li>Sampling: 110 Hz / 55 Hz</li> </ul> <p>Data was segmented into 1-second windows, with simple features (mean, min, max) computed per axis and sensor — enabling efficient modeling with limited compute.</p> <hr> <h3 id="-signal-patterns-across-users">📉 Signal Patterns Across Users</h3> <div class="row"> <div class="col-sm-12 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/3dplots-480.webp 480w,/assets/img/projects/1_project/3dplots-800.webp 800w,/assets/img/projects/1_project/3dplots-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/3dplots.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="3D IMU Traces by User" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Variability in gesture patterns across users formed the basis for behavioral biometric classification — essential for reliable authentication and identification.</p> <hr> <h3 id="-results">📊 Results</h3> <div class="row"> <div class="col-sm-6 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/classifiers-480.webp 480w,/assets/img/projects/1_project/classifiers-800.webp 800w,/assets/img/projects/1_project/classifiers-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/classifiers.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Classifier Comparison" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/far_frr_eer-480.webp 480w,/assets/img/projects/1_project/far_frr_eer-800.webp 800w,/assets/img/projects/1_project/far_frr_eer-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/far_frr_eer.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="EER / FAR / FRR per Gesture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>Authentication</strong>: Achieved 1.3% EER using triangle gesture and RotVec + GeoMag sensors</li> <li> <strong>Identification</strong>: 99.3% F1-score with Random Forest using 3-sensor fusion</li> <li>Adaboost outperformed other models in most gesture-sensor settings</li> <li>Applied sliding window overlap and SMOTE to handle sequential structure and class imbalance</li> </ul> <div class="row"> <div class="col-sm-6 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/participant_based2-480.webp 480w,/assets/img/projects/1_project/participant_based2-800.webp 800w,/assets/img/projects/1_project/participant_based2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/participant_based2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Error Reduction with Optimizations" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/eer_results-480.webp 480w,/assets/img/projects/1_project/eer_results-800.webp 800w,/assets/img/projects/1_project/eer_results-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/eer_results.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Sensor Fusion Results" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <h3 id="-highlights">✅ Highlights</h3> <ul> <li>End-to-end ML pipeline for user presence detection, authentication, and identification</li> <li>Real-time classification using <strong>only 3 handcrafted features</strong> per signal (mean, min, max)</li> <li>Demonstrated <strong>state-of-the-art results</strong>: 1.3% EER and 99.3% identification F1-score</li> <li>Outperformed baselines using <strong>Adaboost and minimal sensor fusion</strong> </li> <li>Optimized for <strong>resource-limited, real-time execution</strong> on wearable hardware</li> <li>Published in a peer-reviewed journal: <em>SN Computer Science (Springer, 2023)</em> </li> </ul> <hr> <h3 id="-resources">🔗 Resources</h3> <ul> <li>📁 <a href="https://github.com/sumeyye-agac/glass-data-participant-detection" rel="external nofollow noopener" target="_blank">GitHub Repository</a> </li> <li>📄 <a href="https://doi.org/10.1007/s42979-023-02202-4" rel="external nofollow noopener" target="_blank">Published Paper (SN Computer Science)</a> </li> </ul> <hr> <h3 id="-technical-summary">🧾 Technical Summary</h3> <ul> <li>📌 <strong>Problem:</strong> Detect whether smart glasses are being worn and by whom — to reduce noise and support downstream HAR/authentication tasks</li> <li>📦 <strong>Dataset:</strong> 17 participants, 6 gestures, 4 IMU sensors → 36 raw signals, 108 statistical features</li> <li>⚙️ <strong>Methods:</strong> <ul> <li>Classifiers: Adaboost (best), RF, SVM, MLP + ensembles</li> <li>Preprocessing: downsampling, normalization, 50% sliding windows</li> <li>Oversampling: SMOTE for minority class (user)</li> <li>Validation: 10-fold cross-validation across users and gestures</li> </ul> </li> <li>📈 <strong>Performance:</strong> <ul> <li>1.3% Equal Error Rate with Adaboost</li> <li>99.3% weighted F1-score in user identification</li> <li>Best results achieved using only RotVec + GeoMag (2 sensors)</li> </ul> </li> </ul> <hr> <p><strong>Tags:</strong> <code class="language-plaintext highlighter-rouge">#MachineLearning</code> <code class="language-plaintext highlighter-rouge">#Wearables</code> <code class="language-plaintext highlighter-rouge">#EdgeAI</code> <code class="language-plaintext highlighter-rouge">#SensorFusion</code> <code class="language-plaintext highlighter-rouge">#IMU</code> <code class="language-plaintext highlighter-rouge">#BehavioralBiometrics</code> <code class="language-plaintext highlighter-rouge">#TimeSeriesClassification</code> <code class="language-plaintext highlighter-rouge">#UserAuthentication</code> <code class="language-plaintext highlighter-rouge">#Identification</code></p> </body></html>