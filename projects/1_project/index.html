<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h3 id="-smart-glass-participant-detection">👓 Smart Glass Participant Detection</h3> <p><strong>A lightweight, on-device model to detect user presence using smart glasses’ motion sensors.</strong></p> <hr> <h3 id="-why-it-matters">🚀 Why It Matters</h3> <p>Wearables continuously generate sensor data — but not all of it is meaningful. This project tackles the practical problem of detecting whether a smart glass device is actively being worn. It ensures that downstream applications like human activity recognition (HAR) or user authentication don’t process irrelevant or noisy data.</p> <hr> <h3 id="️-system-overview">⚙️ System Overview</h3> <div class="row"> <div class="col-sm-12 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/architecture-480.webp 480w,/assets/img/projects/1_project/architecture-800.webp 800w,/assets/img/projects/1_project/architecture-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/architecture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="System Pipeline" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We collect IMU data from smart glasses, extract low-cost features, and classify each segment as “user” or “no-user” — all on-device, with real-time capability.</p> <hr> <h3 id="-dataset--gesture-design">🧪 Dataset &amp; Gesture Design</h3> <div class="row"> <div class="col-sm-12 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/head_movements-480.webp 480w,/assets/img/projects/1_project/head_movements-800.webp 800w,/assets/img/projects/1_project/head_movements-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/head_movements.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Gesture Set" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>17 participants</strong>, each performing 6 gestures</li> <li> <strong>Device</strong>: Epson Moverio BT-350</li> <li> <strong>Sensors</strong>: Accelerometer, Gyroscope, Rotation Vector, Geomagnetic</li> <li> <strong>Sampling</strong>: 110 Hz (Acc/Gyr), 55 Hz (GeoMag/RotVec)</li> </ul> <hr> <h3 id="-sensor-signals-user-variation">📉 Sensor Signals: User Variation</h3> <div class="row"> <div class="col-sm-12 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/3dplots-480.webp 480w,/assets/img/projects/1_project/3dplots-800.webp 800w,/assets/img/projects/1_project/3dplots-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/3dplots.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="3D IMU Traces by User" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Gestures produce distinct patterns across users and sensors — a foundation for both authentication and filtering.</p> <hr> <h3 id="-results-summary">📊 Results Summary</h3> <div class="row"> <div class="col-sm-6 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/classifiers-480.webp 480w,/assets/img/projects/1_project/classifiers-800.webp 800w,/assets/img/projects/1_project/classifiers-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/classifiers.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Classifier Comparison (EER %)" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/far_frr_eer-480.webp 480w,/assets/img/projects/1_project/far_frr_eer-800.webp 800w,/assets/img/projects/1_project/far_frr_eer-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/far_frr_eer.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="EER/FAR/FRR by Gesture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>Adaboost outperforms</strong> RF, SVM, and MLP across all gestures</li> <li>Triangle gesture with RotVec+GeoMag achieves <strong>1.3% EER</strong> </li> </ul> <div class="row"> <div class="col-sm-6 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/participant_based2-480.webp 480w,/assets/img/projects/1_project/participant_based2-800.webp 800w,/assets/img/projects/1_project/participant_based2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/participant_based2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Optimization Impact by User" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/eer_results-480.webp 480w,/assets/img/projects/1_project/eer_results-800.webp 800w,/assets/img/projects/1_project/eer_results-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/eer_results.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Sensor Fusion Results Table" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Oversampling + sliding window reduced EER across users</li> <li>Using <strong>only 2 sensors</strong> yields performance comparable to 3–4 sensor setups</li> </ul> <hr> <h3 id="-key-takeaways">🔑 Key Takeaways</h3> <ul> <li>✅ <strong>1.3% EER</strong> with triangle gesture</li> <li>✅ <strong>Only 3 features per signal</strong>: mean, min, max</li> <li>✅ Runs efficiently on wearable hardware (no deep learning required)</li> <li>✅ Dataset + logger developed in-house</li> <li>✅ Supports further steps like authentication &amp; identification</li> </ul> <hr> <h3 id="-resources">🔗 Resources</h3> <ul> <li>📁 <a href="https://github.com/sumeyye-agac/glass-data-participant-detection" rel="external nofollow noopener" target="_blank">GitHub Repository</a> </li> <li>📄 <a href="https://doi.org/10.1007/s42979-023-02202-4" rel="external nofollow noopener" target="_blank">Published Paper (SN Computer Science)</a> </li> </ul> <p><strong>Tags:</strong> <code class="language-plaintext highlighter-rouge">#EdgeAI</code> <code class="language-plaintext highlighter-rouge">#Wearables</code> <code class="language-plaintext highlighter-rouge">#SensorFusion</code> <code class="language-plaintext highlighter-rouge">#IMU</code> <code class="language-plaintext highlighter-rouge">#BehavioralBiometrics</code> <code class="language-plaintext highlighter-rouge">#MachineLearning</code></p> </body></html>