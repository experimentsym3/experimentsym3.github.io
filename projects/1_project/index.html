<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h3 id="-user-classification-for-smart-glasses-from-authentication-to-identification">👓 User Classification for Smart Glasses: From Authentication to Identification</h3> <p><strong>A real-time, on-device system for detecting user presence, authenticating, and identifying individuals via motion sensor data from smart glasses.</strong></p> <p>This project builds a full pipeline for on-device classification based on IMU data from smart glasses. It enables wearable devices to detect whether they’re being worn — a critical step before authentication, identification, or activity recognition tasks. The system is optimized for embedded hardware using classical ML methods and minimal compute.</p> <hr> <h3 id="-project-summary">🎯 Project Summary</h3> <ul> <li> <strong>Goal:</strong> Enable presence-aware filtering and secure personalization on head-mounted displays</li> <li> <strong>Device:</strong> Epson Moverio BT-350 smart glasses</li> <li> <strong>Sensors:</strong> Accelerometer, Gyroscope, Geomagnetic, Rotation Vector</li> <li> <strong>Data:</strong> 17 participants, 6 gestures, 110/55 Hz sampling</li> <li> <strong>Model Input:</strong> 1-second windows, 108 time-domain features</li> <li> <strong>Output:</strong> Binary classification (worn or not), multiclass user ID</li> </ul> <hr> <h3 id="-system-pipeline">💡 System Pipeline</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/architecture-480.webp 480w,/assets/img/projects/1_project/architecture-800.webp 800w,/assets/img/projects/1_project/architecture-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/architecture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="System Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The system segments IMU signals into 1-second windows and classifies user presence. When a user is detected, the system supports authentication and identification. </div> <hr> <h3 id="-gesture-design--data-collection">📐 Gesture Design &amp; Data Collection</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/head_movements-480.webp 480w,/assets/img/projects/1_project/head_movements-800.webp 800w,/assets/img/projects/1_project/head_movements-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/head_movements.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Gesture Set" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Participants performed 6 head gestures: circle, up-down, tilt, triangle, turn, and square. Each gesture creates distinct IMU patterns across users. </div> <hr> <h3 id="-user-level-signal-variation">🔍 User-Level Signal Variation</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/3dplots-480.webp 480w,/assets/img/projects/1_project/3dplots-800.webp 800w,/assets/img/projects/1_project/3dplots-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/3dplots.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="3D IMU Signal Patterns" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Example 3D traces show user-dependent variation — a key for biometric classification. </div> <hr> <h3 id="-classifier-performance">📊 Classifier Performance</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/classifiers-480.webp 480w,/assets/img/projects/1_project/classifiers-800.webp 800w,/assets/img/projects/1_project/classifiers-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/classifiers.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Classifier Comparison" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/far_frr_eer-480.webp 480w,/assets/img/projects/1_project/far_frr_eer-800.webp 800w,/assets/img/projects/1_project/far_frr_eer-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/far_frr_eer.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Authentication Error Metrics" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Adaboost consistently achieved lowest error rates across gestures. Best authentication result: 1.3% EER with triangle gesture using RotVec + GeoMag. </div> <hr> <h3 id="-model-optimization">🔧 Model Optimization</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/participant_based2-480.webp 480w,/assets/img/projects/1_project/participant_based2-800.webp 800w,/assets/img/projects/1_project/participant_based2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/participant_based2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Error Reduction After SMOTE + Sliding Window" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/1_project/eer_results-480.webp 480w,/assets/img/projects/1_project/eer_results-800.webp 800w,/assets/img/projects/1_project/eer_results-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/1_project/eer_results.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Sensor Fusion Results" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Using only 2 sensors (RotVec + GeoMag) and a simple triangle gesture provided robust results — suitable for deployment on embedded hardware. </div> <hr> <h3 id="-technical-stack--contributions">🧠 Technical Stack &amp; Contributions</h3> <ul> <li>Time-Series Segmentation (1s windows with 50% overlap)</li> <li>Feature Extraction: mean, min, max per axis (no deep features)</li> <li>Classifiers: Adaboost (best), RF, SVM (poly/RBF), MLP, model ensembles</li> <li>Class imbalance handled via <code>SMOTE</code> </li> <li>Evaluated using 10-fold cross-validation across participants</li> </ul> <hr> <h3 id="-outcomes">✅ Outcomes</h3> <ul> <li>🔐 <strong>Authentication</strong>: 1.3% EER</li> <li>🧍 <strong>Identification</strong>: 99.3% weighted F1-score</li> <li>⚡ <strong>Optimized for wearables</strong>: no deep learning, no high-power processing</li> <li>📰 <strong>Published</strong> in <em>SN Computer Science (Springer, 2023)</em> </li> </ul> <hr> <h3 id="-resources">🔗 Resources</h3> <ul> <li>📁 <a href="https://github.com/sumeyye-agac/glass-data-participant-detection" target="_blank" rel="external nofollow noopener">GitHub Repository</a> </li> <li>📄 <a href="https://doi.org/10.1007/s42979-023-02202-4" target="_blank" rel="external nofollow noopener">Published Paper</a> </li> </ul> <hr> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3"> <p><strong>Tags:</strong></p> <ul> <li> <code>#UserAuthentication</code> – Lightweight IMU-based classification with 1.3% EER</li> <li> <code>#Identification</code> – Achieved 99.3% F1-score using behavioral biometrics</li> <li> <code>#EdgeAI</code> – Runs on-device with minimal compute and handcrafted features</li> <li> <code>#SensorFusion</code> – Used RotVec + GeoMag for optimal balance of performance and efficiency</li> <li> <code>#TimeSeriesClassification</code> – Sliding window + SMOTE for sequential signal learning</li> <li> <code>#WearableComputing</code> – System built and tested on real smart glass hardware</li> </ul> </div> </div> </body></html>